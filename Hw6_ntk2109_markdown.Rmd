---
title: "HW6_ntk2109_Markdown"
author: "Noah Kreski"
date: "November 20, 2018"
output: github_document
---
###Problem One: Homicide Regression
```{r setup, include=FALSE}
library(tidyverse)
library(forcats)
library(modelr)
library(mgcv)
```
```{r Problem One Data, message=FALSE, warning=FALSE}
 #I am creating city_state, eliminating data errors and unnecessary city-states, convert age to numeric, and applying a true/false statement to non-unknown race
Washington_data = read_csv("./data/WashingtonPost/homicide-data.csv")%>%
                  mutate(city_state = paste(city,state, sep = ","))%>%
                  filter(city_state != "Tulsa,AL")%>%
                  filter(!(city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO")))%>%
                  mutate(victim_age = as.numeric(victim_age))%>%
                  filter((victim_race != "Unknown"))%>%
                  mutate(victim_race = as.numeric(victim_race == "White"))
#I am assigning white and non-white labels to race and numeric values to whether a case was resolved, while also making white the reference                  
Washington_data$victim_race <- factor(Washington_data$victim_race, levels = c(1,0), labels = c("White","Non-White"))
Washington_data$Solved <- as.numeric(Washington_data$disposition == "Closed by arrest")
Washington_data$victim_race = fct_relevel(Washington_data$victim_race, "White")

```

The above tidying prepares the data for regression analysis solved vs unresolved as the outcome and victim age, sex and race as predictors.

```{r Problem One GLM Baltimore, message=FALSE}
#I am modeling the solved outcome for Baltimore logistically.
fit_logistic = 
  Washington_data%>% 
  filter(city_state == "Baltimore,MD")%>%
  glm(Solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
#I am getting the standard error for the regression coefficient of race
Race_se = (coef(summary(fit_logistic))[3, 2])
          
#I am tidying the data to get what I need, and creating a 95% confidence interval with the standard error attained earlier.
  fit_logistic %>%
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  mutate(conf_lower = exp(estimate - 1.96 * Race_se))%>%
  mutate(conf_higher = exp(estimate + 1.96 * Race_se))%>%
  select(term, log_OR = estimate, OR, conf_lower, conf_higher)%>%
  filter(term == "victim_raceNon-White")

```
The odds ratio for solving homicides comparing non-white victims to white victims, controlling for age and sex, in Baltimore, Md is .440608, with a 95% confidence interval of (.3129079, .6204234).

```{r Problem One GLM All cities, message=FALSE}


odds_function = function(x) {
  
  #I am modeling the solved outcome for every city logistically.
fit_logistic = 
  Washington_data%>% 
  filter(city_state == x)%>%
  glm(Solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
#I am getting the standard error for the regression coefficient of race
Race_se = (coef(summary(fit_logistic))[3, 2])
          
#I am tidying the data to get what I need, and creating a 95% confidence interval with the standard error attained earlier.
  fit_logistic %>%
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  mutate(conf_lower = exp(estimate - 1.96 * Race_se))%>%
  mutate(conf_higher = exp(estimate + 1.96 * Race_se))%>%
  select(term, log_OR = estimate, OR, conf_lower, conf_higher)%>%
  filter(term == "victim_raceNon-White")
}
#This code will iterate the function over all cities, produce a column with tibbles, and unnest to generate proportions and CIs for every city.
city_list = c(unique(Washington_data$city_state))
city_df = tibble(city_list,
map(city_list, odds_function)) %>%
unnest()

city_df
```

The above data frame provides the odds ratios for solving homicides comparing non-white to white victims, both as a log estimate and the odds ratio, with confidence intervals.

```{r graph OR cities}
city_df%>%
mutate(city_list = forcats::fct_reorder(city_list, OR))%>%
#I am creating a visual with proportions by location, with error bars, and formatting appropriately.
ggplot( aes(x = city_list, y = OR, group = city_list)) + geom_point() +theme(axis.text.x = element_text(angle = 70, hjust = 1)) + geom_errorbar( mapping=aes(x=city_list, ymin=conf_lower, ymax=conf_higher))+labs(title = "ORs for Solved Homicides, NonWhite vs White Victim, Adjusted by Sex/Age", x = "Location", y = "Odds Ratio")
  
```

This plot shows the adjusted odds ratios comparing Nonwhite to White homicide victims in terms of their homicides being solved. Overwhelmingly, Non-White victims were less likely to have their homicides solved than White victims, and approximately half of these odds ratios had a confidence interval that did not contain the null value, an odds ratio of 1.00. This provides consistent evidence of disparities in this outcome.

###Problem Two: Child Birthweight Models

To begin, I will conduct a backwards regression on the full set of variables. While this may be less thorough than other methods in isolation, it is only to supply a reasonable subset of variables that are critical to the outcome in question, which will then be honed with theory and conceptual edits.

```{r Problem Two GLM Backwards Selection, message=FALSE, eval=FALSE}
#I am getting the data with appropriate formats
birthweight_data = read_csv("./data/birthweight.csv")%>%
                   mutate(babysex = as.factor(babysex), frace = as.factor(frace), malform = as.factor(malform), mrace = as.factor(mrace))
 #I am using a backwards selection model, eliminating one of the full set of predictors based on whatever has the highest p value until all are below .05                 
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + mrace + momage +parity + pnumlbw + pnumsga +ppbmi + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated frace, all levels above .05
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + malform + menarche + mheight + mrace + momage +parity + pnumlbw + pnumsga +ppbmi + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated malform, p = .889
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + menarche + mheight + mrace + momage +parity + pnumlbw + pnumsga +ppbmi + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated ppbmi, p = .760
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + menarche + mheight + mrace + momage +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated momage, p = .530
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + menarche + mheight + mrace +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated menarche, p = .245
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#Eliminated fincome, p = .069
birthweight_data %>% 
  lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mheight + mrace +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#All variables p<.05 or multiple levels p<.05

```

Following data loading and tidying, creating factors where necessary, I used a backwards selection model to attain a final regression model for these data. Every step I took away to predictor with the highest p-value until all variables had a p<.05, or multiple factor levels below that threshold. This final model has 13 predictors including baby sex, head circumference at birth in cm, length at birth in cm, mother's weight at delivery in pounds, gestational age in weeks, mother's height, mother's race, number of live births prior to this pregnancy, previous number of low birth weight babies, number of prior small for gestational age babies, mother's pre-pregnancy weight, average number of cigarettes smoked per day during pregnancy and weight gain during pregnancy. 

However, it is worth asking conceptually what may be missing from this model. Prior research has shown that there may be an interaction between a baby's sex and growth timeline [see citation below], and so an interaction term between sex amd gestational age is a potentially worthwhile addition to this model.

Citation : Broere-Brown, Z. A., Baan, E., Schalekamp-Timmermans, S., Verburg, B. O., Jaddoe, V. W., & Steegers, E. A. (2016). Sex-specific differences in fetal and infant growth patterns: a prospective population-based cohort study. Biology of sex differences, 7, 65. doi:10.1186/s13293-016-0119-1

Additionally, a smaller gestational age may be more of a concern for low birth weight in mothers with a higher number of prior "small for gestational age" births, and so an interaction term between gaweeks and pnumsga is warranted.

```{r model fit, message=FALSE, warning=FALSE}
#I'm creating the needed data set again in a chunk where eval does not equal False
birthweight_data = read_csv("./data/birthweight.csv")%>%
                   mutate(babysex = as.factor(babysex), frace = as.factor(frace), malform = as.factor(malform), mrace = as.factor(mrace))
#I'm assigning my final model to fit
fit = lm(bwt ~ babysex + blength + bhead + delwt + gaweeks + mheight + mrace +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain + babysex*gaweeks + gaweeks*pnumsga, data = birthweight_data)
#I am plotting the predictions and residuals
birthweight_data%>%
  modelr::add_predictions(fit)%>%
  modelr::add_residuals(fit)%>%
  ggplot(aes(x=pred, y = resid)) + geom_point() + labs(title = "Residuals vs Predicted values: Backwards Selection model")
```


The residuals appear to exhibit consistent behavior across predicted values, centralized at a value of 0 and evenly distributed above and below, with the majority of residuals at an absolute value of 500 or less. 
```{r other models}
#I'm fitting the first alternative model and mapping its predictions and residuals.
fit_length_age = lm(bwt ~ blength + gaweeks, data = birthweight_data)

birthweight_data%>%
  modelr::add_predictions(fit_length_age)%>%
  modelr::add_residuals(fit_length_age)%>%
  ggplot(aes(x=pred, y = resid)) + geom_point() + labs(title = "Residuals vs Predicted values: Length and Age")

#I'm fitting the second alternative model and mapping its predictions and residuals.
fit_circumference_length_sex = lm (bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_data)

birthweight_data%>%
  modelr::add_predictions(fit_circumference_length_sex)%>%
  modelr::add_residuals(fit_circumference_length_sex)%>%
  ggplot(aes(x=pred, y = resid)) + geom_point() + labs(title = "Residuals vs Predicted values: Circumference, Length and Sex")
```

These plots map the residuals and predicted values for the two alternative models in the same way that I did earlier for my own model. The model with just length and age has much larger residuals, often close to 1,000, whereas the model with sex, head circumference and length appears similar to my own model.

```{r Cross-validation, message=FALSE, warning=FALSE}
#I am developing test and train subsets for cross-validation
cv_df = crossv_mc(birthweight_data, 100) 

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble

cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df =
  cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
#I am using my various models in cross-validation examining each model with the train set, and then examining RMSE with the test set
cv_df = 
  cv_df %>% 
  mutate( fit = map(train, ~lm(bwt ~ babysex + blength + bhead +  delwt + gaweeks + mheight + mrace +parity + pnumlbw + pnumsga + ppwt + smoken + wtgain + babysex*gaweeks + gaweeks*pnumsga, data = .x)),
         fit_length_age = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         fit_circumference_length_sex = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_fit    = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_fit_length = map2_dbl(fit_length_age, test, ~rmse(model = .x, data = .y)),
         rmse_fit_circumference = map2_dbl(fit_circumference_length_sex, test, ~rmse(model = .x, data = .y)))
#I am visualizing the RMSE distributions for each model.
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + labs(title = "RMSE by model")
```

The above cross validation and RMSE visual shows a slightly lower RMSE for my own model than for either of the two alternative models provides, which indicates a superior predictive fit. Generally, this suggests that the model I developed is superior to the other two, at least for minimization of error.